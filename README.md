# LLMs-Journey

## What to read?

Books:
- [Foundations of Large Language Models](https://arxiv.org/pdf/2501.09223)
- [How to Scale Your Model](https://jax-ml.github.io/scaling-book/index)

Agents:
- [Agents (Google's whitepaper)](https://www.kaggle.com/whitepaper-agents)

Fine-Tuning:
- [ðŸ¤— PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)
- [QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314)

RAG related:
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [Chain-of-Retrieval Augmented Generation](https://arxiv.org/pdf/2501.14342)
- [Introducing Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)

Evaluation:
- [A Survey on LLM-as-a-Judge](https://arxiv.org/pdf/2411.15594)
- [ARC Prize 2024: Technical Report](https://arxiv.org/pdf/2412.04604)
- [FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI](https://arxiv.org/pdf/2411.04872)

Models:
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)
- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/pdf/2402.03300)
- [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864) and [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/pdf/2410.06205)
- [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://arxiv.org/abs/2409.12191)
- [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL?tab=readme-ov-file)
- [Qwen2.5-Math](https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file) and [paper](https://arxiv.org/abs/2409.12122)

Chain-of-Thought
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)

Test-Time Scaling
- [s1: Simple test-time scaling](https://github.com/simplescaling/s1) and [paper](https://arxiv.org/abs/2501.19393)

Test-Time Comput
- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)

## Resources

MLX:
- [mlx-examples](https://github.com/ml-explore/mlx-examples/tree/main)
- [ml-explore](https://github.com/ml-explore)
- [mlx-vlm](https://github.com/Blaizzy/mlx-vlm)

LangChain:
- [Build a Retrieval Augmented Generation (RAG) App: Part 1](https://python.langchain.com/docs/tutorials/rag/)

LangGraph:
- l

LangSmith:
- l

Ollama:
- l

LLamaIndex:
- l

Databases:
- [Chroma](https://www.trychroma.com/home)

HuggingFace:
- [MLX Community](https://huggingface.co/mlx-community)
- [Using MLX at Hugging Face](https://huggingface.co/docs/hub/en/mlx)

Leonie Notebooks:
- [Fine-tuning Gemma 2 JPN for Yomigana with LoRA](https://www.kaggle.com/code/iamleonie/fine-tuning-gemma-2-jpn-for-yomigana-with-lora)
- [Advanced RAG with Gemma, Weaviate, and LlamaIndex](https://www.kaggle.com/code/iamleonie/advanced-rag-with-gemma-weaviate-and-llamaindex)
- [RAG with Gemma on HF ðŸ¤— and Weaviate in DSPy](https://www.kaggle.com/code/iamleonie/rag-with-gemma-on-hf-and-weaviate-in-dspy)

GitHub Repos:
- [rag-cookbooks](https://github.com/athina-ai/rag-cookbooks)
- [Hands-On-Large-Language-Models](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models)
- [Multimodal-RAG-Implementation](https://github.com/CornelliusYW/Multimodal-RAG-Implementation)
- [Data_Science_Learning_Material](https://github.com/CornelliusYW/Data_Science_Learning_Material)
- [RAG-To-Know](https://github.com/CornelliusYW/RAG-To-Know)
- [Docling](https://github.com/DS4SD/docling?tab=readme-ov-file)
