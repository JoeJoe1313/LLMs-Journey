# LLMs-Journey

## What to read?

Books:
- [Foundations of Large Language Models](https://arxiv.org/pdf/2501.09223)
- [How to Scale Your Model](https://jax-ml.github.io/scaling-book/index)
- [The Ultra-Scale Playbook: Training LLMs on GPU Clusters](https://huggingface.co/spaces/nanotron/ultrascale-playbook)

Agents:
- [Hugging Face Agents Course](https://huggingface.co/agents-course)
- [ai-agents-for-beginners (Microsoft)](https://github.com/microsoft/ai-agents-for-beginners)
- [Agents (Google's whitepaper)](https://www.kaggle.com/whitepaper-agents)
- [Agency Is Frame-Dependent](https://arxiv.org/abs/2502.04403)

Fine-Tuning:
- [ðŸ¤— PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)
- [QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314)

RAG related:
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [Chain-of-Retrieval Augmented Generation](https://arxiv.org/pdf/2501.14342)
- [Introducing Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)

Evaluation:
- [A Survey on LLM-as-a-Judge](https://arxiv.org/pdf/2411.15594)
- [ARC Prize 2024: Technical Report](https://arxiv.org/pdf/2412.04604)
- [FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI](https://arxiv.org/pdf/2411.04872)
- [MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations](https://arxiv.org/abs/2502.06453v1)

Datasets:
- [NuminaMath 1.5](https://huggingface.co/datasets/AI-MO/NuminaMath-1.5)

Models:
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)
- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/pdf/2402.03300)
- [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864) and [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/pdf/2410.06205)
- [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://arxiv.org/abs/2409.12191)
- [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL?tab=readme-ov-file) and [paper](https://arxiv.org/abs/2502.13923)
- [Qwen2.5-Math](https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file) and [paper](https://arxiv.org/abs/2409.12122)
- [PaliGemma 2: A Family of Versatile VLMs for Transfer](https://arxiv.org/abs/2412.03555v1)

Chain-of-Thought
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)

Visualisation-of-Thought:
- [Imagine while Reasoning in Space: Multimodal Visualization-of-Thought](https://arxiv.org/pdf/2501.07542)

Test-Time Scaling
- [s1: Simple test-time scaling](https://github.com/simplescaling/s1) and [paper](https://arxiv.org/abs/2501.19393)

Test-Time Compute
- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)
- [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171)

AlphaGeometry:
- [Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2](https://arxiv.org/abs/2502.03544)

Apple:
- [Machine Learning Research at Apple](https://machinelearning.apple.com)
- [Distillation Scaling Laws](https://arxiv.org/abs/2502.08606)
- [Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models](https://arxiv.org/abs/2501.12370)

Misc:
- [Scaling Pre-training to One Hundred Billion Data for Vision Language Models](https://arxiv.org/abs/2502.07617)
- [Competitive Programming with Large Reasoning Models](https://arxiv.org/abs/2502.06807)
- [MoBA: Mixture of Block Attention for Long-Context LLMs](https://arxiv.org/abs/2502.13189)
- [Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model](https://arxiv.org/abs/2502.10248), [repo](https://github.com/stepfun-ai/Step-Video-T2V)
- [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)
- [Deepseek Papers](https://huggingface.co/collections/Presidentlin/deepseek-papers-674c536aa6acddd9bc98c2ac)
- [InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU](https://arxiv.org/abs/2502.08910)
- [Leveraging the true depth of LLMs](https://arxiv.org/abs/2502.02790)
- [NOLIMA: Long-Context Evaluation Beyond Literal Matching](https://arxiv.org/pdf/2502.05167)
- [Memory Layers at Scale](https://arxiv.org/abs/2412.09764)

## Resources

MLX:
- [mlx-examples](https://github.com/ml-explore/mlx-examples/tree/main)
- [ml-explore](https://github.com/ml-explore)
- [mlx-vlm](https://github.com/Blaizzy/mlx-vlm)

LangChain:
- [Build a Retrieval Augmented Generation (RAG) App: Part 1](https://python.langchain.com/docs/tutorials/rag/)

LangGraph:
- l

LangSmith:
- l

Ollama:
- l

LLamaIndex:
- l

Databases:
- [Chroma](https://www.trychroma.com/home)

HuggingFace:
- [MLX Community](https://huggingface.co/mlx-community)
- [Using MLX at Hugging Face](https://huggingface.co/docs/hub/en/mlx)

Leonie Notebooks:
- [Fine-tuning Gemma 2 JPN for Yomigana with LoRA](https://www.kaggle.com/code/iamleonie/fine-tuning-gemma-2-jpn-for-yomigana-with-lora)
- [Advanced RAG with Gemma, Weaviate, and LlamaIndex](https://www.kaggle.com/code/iamleonie/advanced-rag-with-gemma-weaviate-and-llamaindex)
- [RAG with Gemma on HF ðŸ¤— and Weaviate in DSPy](https://www.kaggle.com/code/iamleonie/rag-with-gemma-on-hf-and-weaviate-in-dspy)

GitHub Repos:
- [rag-cookbooks](https://github.com/athina-ai/rag-cookbooks)
- [Hands-On-Large-Language-Models](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models)
- [Multimodal-RAG-Implementation](https://github.com/CornelliusYW/Multimodal-RAG-Implementation)
- [Data_Science_Learning_Material](https://github.com/CornelliusYW/Data_Science_Learning_Material)
- [RAG-To-Know](https://github.com/CornelliusYW/RAG-To-Know)
- [Docling](https://github.com/DS4SD/docling?tab=readme-ov-file)
