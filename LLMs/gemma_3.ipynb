{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U mlx mlx-lm\n",
    "# !pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "from mlx_lm import generate, load\n",
    "from mlx_lm.chat import load, make_prompt_cache, make_sampler, stream_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mlx-community/gemma-3-4b-it-8bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create args namespace with custom values\n",
    "args = SimpleNamespace(\n",
    "    model=model_name,\n",
    "    adapter_path=None,\n",
    "    temp=0.7,\n",
    "    top_p=0.9,\n",
    "    seed=None,\n",
    "    max_kv_size=None,\n",
    "    max_tokens=2_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b08e4aa9640446c916b00dad889a312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize model and tokenizer\n",
    "model, tokenizer = load(\n",
    "    args.model,\n",
    "    adapter_path=args.adapter_path,\n",
    "    tokenizer_config={\"trust_remote_code\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cache = make_prompt_cache(model, args.max_kv_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query):\n",
    "    response_text = \"\"\n",
    "    for response in stream_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": query}], add_generation_prompt=True),\n",
    "        max_tokens=args.max_tokens,\n",
    "        sampler=make_sampler(args.temp, args.top_p),\n",
    "        prompt_cache=prompt_cache\n",
    "    ):\n",
    "        print(response.text, end='', flush=True)\n",
    "        response_text += response.text\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interface():\n",
    "    # Textarea for better paste support\n",
    "    text_input = widgets.Textarea(\n",
    "        placeholder='Type or paste your question here...',\n",
    "        description='Query:',\n",
    "        layout=widgets.Layout(\n",
    "            width='50%',\n",
    "            height='100px'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add a Submit button since Textarea doesn't have on_submit\n",
    "    submit_button = widgets.Button(\n",
    "        description='Submit',\n",
    "        layout=widgets.Layout(width='100px')\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    conversation_history = []\n",
    "    \n",
    "    def on_submit(b):\n",
    "        with output_area:\n",
    "            query = text_input.value\n",
    "            if not query.strip():\n",
    "                return\n",
    "                \n",
    "            clear_output()\n",
    "            \n",
    "            for _, exchange in enumerate(conversation_history, 1):\n",
    "                print(f\"\\nQ: {exchange['Q']}\\n\")\n",
    "                print(f\"A: {exchange['A']}\\n\")\n",
    "                print(\"-\" * 50)\n",
    "            \n",
    "            print(f\"\\nQ: {query}\\n\")\n",
    "            print(\"A: \", end='', flush=True)\n",
    "            response_text = get_response(query)\n",
    "            print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "            conversation_history.append({\"Q\": query, \"A\": response_text})\n",
    "            \n",
    "        text_input.value = ''\n",
    "    \n",
    "    submit_button.on_click(on_submit)\n",
    "    \n",
    "    # Create input container with textarea and button side by side\n",
    "    input_container = widgets.HBox([text_input, submit_button])\n",
    "    \n",
    "    # Stack output above input\n",
    "    vbox = widgets.VBox([output_area, input_container])\n",
    "    display(vbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542cdf2aae884dd3a1fcf3041a2ce327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), HBox(children=(Textarea(value='', description='Query:', layout=Layout(height='100px',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
