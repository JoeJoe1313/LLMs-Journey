{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm import generate, load\n",
    "from mlx_lm.tuner import TrainingArgs, datasets, linear_to_lora_layers, train\n",
    "from transformers import PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_load_hf_dataset(\n",
    "    data_id: str,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    names: Tuple[str, str, str] = (\"train\", \"valid\", \"test\"),\n",
    "):\n",
    "    from datasets import exceptions, load_dataset\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(data_id)\n",
    "\n",
    "        train, valid, test = [\n",
    "            (\n",
    "                datasets.create_dataset(dataset[n], tokenizer)\n",
    "                if n in dataset.keys()\n",
    "                else []\n",
    "            )\n",
    "            for n in names\n",
    "        ]\n",
    "\n",
    "    except exceptions.DatasetNotFoundError:\n",
    "        raise ValueError(f\"Not found Hugging Face dataset: {data_id} .\")\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module mlx_lm.tuner.datasets in mlx_lm.tuner:\n",
      "\n",
      "NAME\n",
      "    mlx_lm.tuner.datasets\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        ChatDataset\n",
      "        CompletionsDataset\n",
      "        Dataset\n",
      "    \n",
      "    class ChatDataset(builtins.object)\n",
      "     |  ChatDataset(data: List[Dict[str, str]], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer)\n",
      "     |  \n",
      "     |  A dataset for chat data in the format of {\"messages\": [...]}\n",
      "     |  https://platform.openai.com/docs/guides/fine-tuning/example-format\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx: int)\n",
      "     |  \n",
      "     |  __init__(self, data: List[Dict[str, str]], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CompletionsDataset(builtins.object)\n",
      "     |  CompletionsDataset(data: List[Dict[str, str]], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, prompt_key: str, completion_key: str)\n",
      "     |  \n",
      "     |  A dataset for prompt-completion data in the format of {\"prompt\": ..., \"completion\": ...}\n",
      "     |  or using user-provided keys for prompt and completion values\n",
      "     |  https://platform.openai.com/docs/guides/fine-tuning/example-format\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx: int)\n",
      "     |  \n",
      "     |  __init__(self, data: List[Dict[str, str]], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, prompt_key: str, completion_key: str)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Dataset(builtins.object)\n",
      "     |  Dataset(data: List[Dict[str, str]], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, text_key: str = 'text')\n",
      "     |  \n",
      "     |  Light-weight wrapper to hold a dataset.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx: int)\n",
      "     |  \n",
      "     |  __init__(self, data: List[Dict[str, str]], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, text_key: str = 'text')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    create_dataset(data, tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, prompt_feature: Optional[str] = None, completion_feature: Optional[str] = None)\n",
      "    \n",
      "    load_custom_hf_dataset(args, tokenizer: transformers.tokenization_utils.PreTrainedTokenizer)\n",
      "    \n",
      "    load_dataset(args, tokenizer: transformers.tokenization_utils.PreTrainedTokenizer)\n",
      "    \n",
      "    load_hf_dataset(data_id: str, tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, prompt_feature: Optional[str] = None, completion_feature: Optional[str] = None)\n",
      "    \n",
      "    load_local_dataset(data_path: pathlib.Path, tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, prompt_feature: Optional[str] = None, completion_feature: Optional[str] = None)\n",
      "\n",
      "DATA\n",
      "    Dict = typing.Dict\n",
      "        A generic version of dict.\n",
      "    \n",
      "    List = typing.List\n",
      "        A generic version of list.\n",
      "    \n",
      "    Optional = typing.Optional\n",
      "        Optional type.\n",
      "        \n",
      "        Optional[X] is equivalent to Union[X, None].\n",
      "\n",
      "FILE\n",
      "    /Users/ljoana/.pyenv/versions/imp/lib/python3.9/site-packages/mlx_lm/tuner/datasets.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab3522e367a49aca955663bc8e54d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"mlx-community/gemma-2-2b-it-fp16\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported data format, check the supported formats here:\nhttps://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJofthomas/hermes-function-calling-thinking-V1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_set, val_set, test_set \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_load_hf_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mcustom_load_hf_dataset\u001b[0;34m(data_id, tokenizer, names)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m load_dataset(data_id)\n\u001b[0;32m---> 11\u001b[0m     train, valid, test \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m         (\n\u001b[1;32m     13\u001b[0m             datasets\u001b[38;5;241m.\u001b[39mcreate_dataset(dataset[n], tokenizer)\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m names\n\u001b[1;32m     18\u001b[0m     ]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDatasetNotFoundError:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot found Hugging Face dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m .\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m load_dataset(data_id)\n\u001b[1;32m     11\u001b[0m     train, valid, test \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m         (\n\u001b[0;32m---> 13\u001b[0m             \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m names\n\u001b[1;32m     18\u001b[0m     ]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDatasetNotFoundError:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot found Hugging Face dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m .\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/imp/lib/python3.9/site-packages/mlx_lm/tuner/datasets.py:100\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(data, tokenizer, prompt_feature, completion_feature)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(data, tokenizer)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported data format, check the supported formats here:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported data format, check the supported formats here:\nhttps://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#data."
     ]
    }
   ],
   "source": [
    "dataset_path = \"Jofthomas/hermes-function-calling-thinking-V1\"\n",
    "train_set, val_set, test_set = custom_load_hf_dataset(\n",
    "    data_id=dataset_path,\n",
    "    tokenizer=tokenizer,\n",
    "    names=(\"train\",),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/imp/lib/python3.9/site-packages/mlx_lm/tuner/datasets.py:199\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(args, tokenizer)\u001b[0m\n\u001b[1;32m    197\u001b[0m     train, valid, test \u001b[38;5;241m=\u001b[39m load_custom_hf_dataset(args, tokenizer)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m Path(\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m)\n\u001b[1;32m    201\u001b[0m     prompt_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(args, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    202\u001b[0m     completion_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(args, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "datasets.load_dataset(dataset_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
