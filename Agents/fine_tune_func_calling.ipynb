{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "from datasets import load_dataset\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm import generate, load\n",
    "from mlx_lm.tuner import TrainingArgs, datasets, linear_to_lora_layers, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2449411ad984122abae158a57c6acb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"mlx-community/gemma-2-2b-it-4bit\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Jofthomas/hermes-function-calling-thinking-V1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    messages = sample[\"messages\"]\n",
    "    first_message = messages[0]\n",
    "\n",
    "    # Instead of adding a system message, we merge the content into the first user message\n",
    "    if first_message[\"role\"] == \"system\":\n",
    "        system_message_content = first_message[\"content\"]\n",
    "        # Merge system content with the first user message\n",
    "        messages[1][\"content\"] = (\n",
    "            system_message_content\n",
    "            + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\"\n",
    "            + messages[1][\"content\"]\n",
    "        )\n",
    "        # Remove the system message from the conversation\n",
    "        messages.pop(0)\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 3570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 3570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"conversations\", \"messages\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3213\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 357\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, remove_columns=\"messages\")\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert an amount from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to be converted'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'play_music', 'description': 'Play music', 'parameters': {'type': 'object', 'properties': {'song': {'type': 'string', 'description': 'The name of the song to play'}, 'artist': {'type': 'string', 'description': 'The name of the artist'}}, 'required': ['song']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "Hi, I need to convert 500 USD to Euros. Can you help me with that?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, I need to help the user convert 500 USD to Euros. Let me think about how to approach this.\n",
      "\n",
      "The user mentioned converting 500 USD to EUR, so I know I need to use the 'convert_currency' function. I should check the available functions to make sure I can perform this action. Yes, the function signature shows it requires 'amount', 'from_currency', and 'to_currency'. Perfect, I can plug in 500, 'USD', and 'EUR' respectively.\n",
      "\n",
      "I also need to make sure that I'm only using the required parameters. The function call doesn't need any other details, so I'll structure the JSON accordingly. Let me format it correctly with the amount as a number and the currencies as strings.\n",
      "\n",
      "I think that's all. Time to send the function call with these parameters to get the conversion for the user.\n",
      "</think><tool_call>\n",
      "{'name': 'convert_currency', 'arguments': {'amount': 500, 'from_currency': 'USD', 'to_currency': 'EUR'}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'converted_amount': 425.5, 'from_currency': 'USD', 'to_currency': 'EUR'}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Sure, 500 USD is approximately 425.50 Euros.<end_of_turn><eos>\n",
      "<start_of_turn>human\n",
      "Great, thanks! Now, can you convert 1000 Euros back to USD?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Alright, let me walk through my thought process step by step to understand why I arrived at the function call in the next model message.\n",
      "\n",
      "First, I look at the previous messages to grasp the context. The user initially asked to convert 500 USD to Euros, and the model correctly called the 'convert_currency' function with the appropriate arguments. The tool responded with the converted amount, and the model provided the result back to the user.\n",
      "\n",
      "Now, the user's new message is, \"Great, thanks! Now, can you convert 1000 Euros back to USD?\" My goal is to determine the appropriate function call for this request.\n",
      "\n",
      "I analyze the user's query. They're asking for a conversion from Euros to USD, specifically 1000 Euros. This situation mirrors the previous currency conversion request but with the currencies reversed. The function 'convert_currency' still seems appropriate since it handles conversions between two currencies. The parameters required for this function are 'amount', 'from_currency', and 'to_currency'.\n",
      "\n",
      "I note that the amount to convert is 1000, which is a number, and the currencies are 'EUR' and 'USD', both strings. These fit the parameter types expected by the function. The required parameters are all present, so there's no need for additional information.\n",
      "\n",
      "I consider if there's any other function available. The other function, 'play_music', doesn't apply here since the request is about currency conversion. Therefore, sticking with 'convert_currency' is the logical choice.\n",
      "\n",
      "Next, I structure the function call according to the JSON schema provided. The function name is 'convert_currency', and the arguments include 'amount': 1000, 'from_currency': 'EUR', and 'to_currency': 'USD'. This setup ensures the tool will process the conversion accurately.\n",
      "\n",
      "I also verify that I'm not making any unnecessary assumptions. The tool requires only the three parameters, and all are provided without extra details. This keeps the function call efficient and focused solely on the user's request.\n",
      "\n",
      "In summary, the user's new query is a direct currency conversion request, which aligns perfectly with the capabilities of the 'convert_currency' function. By supplying the correct amount and currency codes, the function will provide the necessary conversion result for the user.\n",
      "</think><tool_call>\n",
      "{'name': 'convert_currency', 'arguments': {'amount': 1000, 'from_currency': 'EUR', 'to_currency': 'USD'}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'converted_amount': 1176.47, 'from_currency': 'EUR', 'to_currency': 'USD'}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "1000 Euros is approximately 1176.47 USD.<end_of_turn><eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.pad_token)\n",
    "# print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ChatmlSpecialTokens(str, Enum):\n",
    "#     tools = \"<tools>\"\n",
    "#     eotools = \"</tools>\"\n",
    "#     think = \"<think>\"\n",
    "#     eothink = \"</think>\"\n",
    "#     tool_call=\"<tool_call>\"\n",
    "#     eotool_call=\"</tool_call>\"\n",
    "#     tool_response=\"<tool_reponse>\"\n",
    "#     eotool_response=\"</tool_reponse>\"\n",
    "#     pad_token = \"<pad>\"\n",
    "#     eos_token = \"<eos>\"\n",
    "#     @classmethod\n",
    "#     def list(cls):\n",
    "#         return [c.value for c in cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = ChatmlSpecialTokens.pad_token.value\n",
    "# tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.additional_special_tokens = ChatmlSpecialTokens.list()\n",
    "# tokenizer.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"adapters_fc\"\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "adapter_config_path = os.path.join(adapter_path, \"adapter_config.json\")\n",
    "adapter_file_path = os.path.join(adapter_path, \"adapters.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "    \"num_layers\": 8,\n",
    "    \"lora_parameters\": {\n",
    "        \"rank\": 16,\n",
    "        \"scale\": 64,\n",
    "        \"dropout\": 0.05,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(adapter_config_path, \"w\") as f:\n",
    "    json.dump(lora_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArgs(\n",
    "    adapter_file=adapter_file_path,\n",
    "    iters=1,\n",
    "    steps_per_eval=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 983040\n"
     ]
    }
   ],
   "source": [
    "num_train_params = sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self) -> None:\n",
    "        self.train_losses: List[Tuple[int, float]] = []\n",
    "        self.val_losses: List[Tuple[int, float]] = []\n",
    "\n",
    "    def on_train_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "\n",
    "    def on_val_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"mask_prompt\": False,\n",
    "    \"prompt_feature\": \"prompt\",\n",
    "    \"text_feature\": \"text\",\n",
    "    \"completion_feature\": \"completion\",\n",
    "    \"chat_feature\": \"messages\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3213\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.create_dataset(\n",
    "    dataset[\"train\"],\n",
    "    tokenizer,\n",
    "    configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = datasets.create_dataset(\n",
    "    dataset[\"test\"],\n",
    "    tokenizer,\n",
    "    configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..., iters: 1\n",
      "Iter 1: Val loss 1.781, Val took 143.580s\n",
      "Iter 1: Train loss 1.746, Learning Rate 1.000e-05, It/sec 0.393, Tokens/sec 145.069, Trained Tokens 3688, Peak mem 20.493 GB\n",
      "Saved final weights to adapters_fc/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    optimizer=optim.Adam(learning_rate=1e-5),\n",
    "    train_dataset=train_set,\n",
    "    val_dataset=val_set,\n",
    "    training_callback=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5240fcb111b44a195547b2652d4629d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_lora, _ = load(model_path, adapter_path=adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): QuantizedEmbedding(256000, 2304, group_size=64, bits=4)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.24): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.25): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'calculate_loan_payment', 'description': 'Calculate the monthly payment for a loan', 'parameters': {'type': 'object', 'properties': {'loan_amount': {'type': 'number', 'description': 'The amount of the loan'}, 'interest_rate': {'type': 'number', 'description': 'The interest rate of the loan'}, 'loan_term': {'type': 'integer', 'description': 'The term of the loan in months'}}, 'required': ['loan_amount', 'interest_rate', 'loan_term']}}}, {'type': 'function', 'function': {'name': 'calculate_tip', 'description': 'Calculate the tip amount for a bill', 'parameters': {'type': 'object', 'properties': {'bill_amount': {'type': 'number', 'description': 'The total bill amount'}, 'tip_percentage': {'type': 'number', 'description': 'The tip percentage to calculate'}}, 'required': ['bill_amount', 'tip_percentage']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "Hi, I need some help with a loan I'm considering.<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Of course, I'd be happy to help. Could you please provide me with the loan amount, the interest rate, and the term of the loan in months?<end_of_turn><eos>\n",
      "<start_of_turn>human\n",
      "Sure, the loan amount is $50000, the interest rate is 5.5%, and the term is 60 months.<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, so the user is asking for help with a loan they're considering. They provided the loan amount, interest rate, and term when I asked for those details. Now, looking at the available functions, I have two options: one to calculate loan payments and another for tips. The loan function requires exactly the details the user provided, so it makes sense to use that. I need to structure the function call with the correct parameters and values. That way, I can accurately compute their monthly payment and provide a helpful response.\n",
      "</think><tool_call>\n",
      "{'name': 'calculate_loan_payment', 'arguments': {'loan_amount': 50000, 'interest_rate': 5.5, 'loan_term': 60}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'monthly_payment': 955.65}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Based on the information you provided, your monthly payment for this loan would be approximately $955.65.<end_of_turn><eos>\n",
      "<start_of_turn>human\n",
      "That's a bit higher than I expected. What would the monthly payment be if I extended the loan term to 72 months?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, so the user is asking for help with a loan they're considering. They initially provided the loan amount, interest rate, and term when I asked for those details. The first function call was to calculate the monthly payment, and it returned $955.65. The user then mentioned that this amount is higher than expected and asked what the payment would be if they extended the term to 72 months. \n",
      "\n",
      "I need to determine the appropriate function to call next. Looking at the available functions, 'calculate_loan_payment' is suitable because the user wants to change the loan term and find out the new monthly payment. The required parameters are loan_amount, interest_rate, and loan_term. All of these values are already provided: $50,000, 5.5%, and now 72 months. \n",
      "\n",
      "So, I should call 'calculate_loan_payment' again but with the updated loan_term. This will give the user the new monthly payment based on the longer term, which might lower the monthly amount as the payments are spread out over more months.\n",
      "</think><tool_call>\n",
      "{'name': 'calculate_loan_payment', 'arguments': {'loan_amount': 50000, 'interest_rate': 5.5, 'loan_term': 72}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'monthly_payment': 805.23}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "If you extend the loan term to 72 months, your monthly payment would decrease to approximately $805.23.<end_of_turn><eos>\n",
      "<start_of_turn>human\n",
      "That sounds more manageable. Thank you for your help!<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "You're welcome! If you have any other questions, feel free to ask.<end_of_turn><eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"<bos><start_of_turn>human\n",
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{tool_call}\n",
    "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
    "\n",
    "Hi, I need to convert 500 USD to Euros. Can you help me with that?<end_of_turn><eos>\n",
    "<start_of_turn>model\n",
    "<think>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "To convert USD to Euros, I need to use the 'convert_currency' function from the provided tools.  I need to provide the amount to convert, the currency to convert from (USD), and the currency to convert to (Euros).  I should also make sure the amount is a number.\n",
      "</think>\n",
      "\n",
      "<tool_call>\n",
      "{\n",
      "  'name': 'convert_currency',\n",
      "  'arguments': {\n",
      "    'amount': 500,\n",
      "    'from_currency': 'USD',\n",
      "    'to_currency': 'EUR'\n",
      "  }\n",
      "}\n",
      "</tool_call> \n",
      "\n",
      "==========\n",
      "Prompt: 460 tokens, 806.327 tokens-per-sec\n",
      "Generation: 135 tokens, 66.724 tokens-per-sec\n",
      "Peak memory: 20.493 GB\n"
     ]
    }
   ],
   "source": [
    "response = generate(model_lora, tokenizer, prompt=prompt, verbose=True, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "Hi, I need to convert 500 USD to Euros. Can you help me with that?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>\n",
      "\n",
      "To convert USD to Euros, I need to use the 'convert_currency' function from the provided tools.  I need to provide the amount to convert, the currency to convert from (USD), and the currency to convert to (Euros).  I should also make sure the amount is a number.\n",
      "</think>\n",
      "\n",
      "<tool_call>\n",
      "{\n",
      "  'name': 'convert_currency',\n",
      "  'arguments': {\n",
      "    'amount': 500,\n",
      "    'from_currency': 'USD',\n",
      "    'to_currency': 'EUR'\n",
      "  }\n",
      "}\n",
      "</tool_call> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
