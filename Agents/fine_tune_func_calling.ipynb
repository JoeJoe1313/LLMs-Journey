{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import mlx.optimizers as optim\n",
    "from datasets import load_dataset\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm import generate, load\n",
    "from mlx_lm.tuner import TrainingArgs, linear_to_lora_layers, train\n",
    "from transformers import PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d044d76ee49d441eab78fafa5e59c121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff04b809558a4eada460f1a0c4667343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  65%|######4   | 954M/1.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"mlx-community/gemma-2-2b-it-4bit\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Jofthomas/hermes-function-calling-thinking-V1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    messages = sample[\"messages\"]\n",
    "    first_message = messages[0]\n",
    "\n",
    "    # Instead of adding a system message, we merge the content into the first user message\n",
    "    if first_message[\"role\"] == \"system\":\n",
    "        system_message_content = first_message[\"content\"]\n",
    "        # Merge system content with the first user message\n",
    "        messages[1][\"content\"] = (\n",
    "            system_message_content\n",
    "            + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\"\n",
    "            + messages[1][\"content\"]\n",
    "        )\n",
    "        # Remove the system message from the conversation\n",
    "        messages.pop(0)\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 3570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 3570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"conversations\", \"messages\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcdd88982364ccfb0c054247b898fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3213\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 357\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, remove_columns=\"messages\")\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'create_todo', 'description': 'Create a new todo item', 'parameters': {'type': 'object', 'properties': {'task': {'type': 'string', 'description': 'The task description'}, 'due_date': {'type': 'string', 'format': 'date', 'description': 'The due date of the task'}, 'priority': {'type': 'integer', 'description': 'The priority of the task (1-5)'}}, 'required': ['task', 'due_date']}}}, {'type': 'function', 'function': {'name': 'calculate_bmi', 'description': 'Calculate the Body Mass Index (BMI)', 'parameters': {'type': 'object', 'properties': {'weight': {'type': 'number', 'description': 'The weight in kilograms'}, 'height': {'type': 'number', 'description': 'The height in meters'}}, 'required': ['weight', 'height']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "I need to create a new task.<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Sure, I can help with that. Could you please provide me with the task description, the due date, and the priority level?<end_of_turn><eos>\n",
      "<start_of_turn>human\n",
      "The task is to prepare a presentation for the annual meeting. The due date is 2022-09-15 and the priority level is 3.<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, so I need to figure out the reasoning that goes between the conversation and the next function call. Let me break this down step by step.\n",
      "\n",
      "First, looking at the conversation, the user starts by saying, \"I need to create a new task.\" The model responds by asking for the task description, due date, and priority level. The user then provides all the necessary details: the task is to prepare a presentation for the annual meeting, the due date is 2022-09-15, and the priority is 3.\n",
      "\n",
      "Now, the model's next move should be to call the appropriate function. The available functions are 'create_todo' and 'calculate_bmi'. Since the user is talking about creating a new task, 'create_todo' is the relevant function here.\n",
      "\n",
      "Examining the function's parameters, it requires 'task', 'due_date', and takes 'priority' as optional. The user provided all three, so we can include them in the arguments.\n",
      "\n",
      "Therefore, the model will execute the 'create_todo' function with the provided task details. This makes sense because the conversation is about setting up a new task, and the function is designed for that exact purpose.\n",
      "</think><tool_call>\n",
      "{'name': 'create_todo', 'arguments': {'task': 'Prepare a presentation for the annual meeting', 'due_date': '2022-09-15', 'priority': 3}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'status': 'success', 'message': 'Todo item successfully created', 'todo_id': '12345'}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Your task has been successfully created. The ID for your new task is 12345.<end_of_turn><eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call=\"<tool_call>\"\n",
    "    eotool_call=\"</tool_call>\"\n",
    "    tool_response=\"<tool_reponse>\"\n",
    "    eotool_response=\"</tool_reponse>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = ChatmlSpecialTokens.pad_token.value\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<tools>',\n",
       " '</tools>',\n",
       " '<think>',\n",
       " '</think>',\n",
       " '<tool_call>',\n",
       " '</tool_call>',\n",
       " '<tool_reponse>',\n",
       " '</tool_reponse>',\n",
       " '<pad>',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.additional_special_tokens = ChatmlSpecialTokens.list()\n",
    "tokenizer.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"adapters_fc\"\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "adapter_config_path = os.path.join(adapter_path, \"adapter_config.json\")\n",
    "adapter_file_path = os.path.join(adapter_path, \"adapters.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "    \"num_layers\": 8,\n",
    "    \"lora_parameters\": {\n",
    "        # \"keys\": [\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"],\n",
    "        \"rank\": 16,\n",
    "        \"scale\": 64,\n",
    "        \"dropout\": 0.05,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(adapter_config_path, \"w\") as f:\n",
    "    json.dump(lora_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArgs(\n",
    "    adapter_file=adapter_file_path,\n",
    "    iters=1,\n",
    "    steps_per_eval=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 983040\n"
     ]
    }
   ],
   "source": [
    "num_train_params = sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self) -> None:\n",
    "        self.train_losses: List[Tuple[int, float]] = []\n",
    "        self.val_losses: List[Tuple[int, float]] = []\n",
    "\n",
    "    def on_train_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "\n",
    "    def on_val_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.tuner import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"mask_prompt\": False,\n",
    "    \"prompt_feature\": \"prompt\",\n",
    "    \"text_feature\": \"text\",\n",
    "    \"completion_feature\": \"completion\",\n",
    "    \"chat_feature\": \"messages\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3213\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.create_dataset(\n",
    "    dataset[\"train\"],\n",
    "    tokenizer,\n",
    "    configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = datasets.create_dataset(\n",
    "    dataset[\"test\"],\n",
    "    tokenizer,\n",
    "    configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..., iters: 1\n",
      "Iter 1: Val loss 1.821, Val took 128.584s\n",
      "Iter 1: Train loss 1.861, Learning Rate 1.000e-05, It/sec 0.430, Tokens/sec 160.427, Trained Tokens 3735, Peak mem 20.665 GB\n",
      "Saved final weights to adapters_fc/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    optimizer=optim.Adam(learning_rate=1e-5),\n",
    "    train_dataset=train_set,\n",
    "    val_dataset=val_set,\n",
    "    training_callback=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef808f0727a49cc95cea2f31e659af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_lora, _ = load(model_path, adapter_path=adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): QuantizedEmbedding(256000, 2304, group_size=64, bits=4)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.24): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.25): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=2048, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (k_proj): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2304, output_dims=1024, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.050000000000000044)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=2048, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=9216, output_dims=2304, bias=False, group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=2304, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'find_nearest_gas_station', 'description': \"Find the nearest gas station based on user's location\", 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The current location'}}, 'required': ['location']}}}, {'type': 'function', 'function': {'name': 'calculate_interest', 'description': 'Calculate the interest on a loan', 'parameters': {'type': 'object', 'properties': {'loan_amount': {'type': 'number', 'description': 'The amount of the loan'}, 'interest_rate': {'type': 'number', 'description': 'The annual interest rate'}, 'loan_term': {'type': 'integer', 'description': 'The term of the loan in years'}}, 'required': ['loan_amount', 'interest_rate', 'loan_term']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "I'm currently in New York City and my car is running out of gas. Can you help me find the nearest gas station?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, so I need to figure out how to help the user find the nearest gas station. The user mentioned they're in New York City and their car is running out of gas. My first thought is that they need a gas station nearby. Looking at the available tools, there's a function called find_nearest_gas_station which seems perfect for this situation. This function requires a location, which the user has provided as 'New York City'. So, I should call this function and pass the location as the argument. That should give the user the nearest gas station they need.\n",
      "</think><tool_call>\n",
      "{'name': 'find_nearest_gas_station', 'arguments': {'location': 'New York City'}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'name': 'find_nearest_gas_station', 'result': {'gas_station_name': 'Shell', 'address': '123 Main St, New York, NY 10001', 'distance': '0.5 miles'}}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "The nearest gas station to your location is Shell, located at 123 Main St, New York, NY 10001. It's just 0.5 miles away from you.<end_of_turn><eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"<bos><start_of_turn>human\n",
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{tool_call}\n",
    "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
    "\n",
    "Hi, I need to convert 500 USD to Euros. Can you help me with that?<end_of_turn><eos>\n",
    "<start_of_turn>model\n",
    "<think>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "To convert USD to Euros, I need to use the 'convert_currency' function from the provided tools.  I need to provide the amount to convert, the currency to convert from (USD), and the currency to convert to (Euros).  I should also make sure the amount is a number.\n",
      "</think>\n",
      "\n",
      "<tool_call>\n",
      "{\n",
      "  'name': 'convert_currency',\n",
      "  'arguments': {\n",
      "    'amount': 500,\n",
      "    'from_currency': 'USD',\n",
      "    'to_currency': 'EUR'\n",
      "  }\n",
      "}\n",
      "</tool_call> \n",
      "\n",
      "==========\n",
      "Prompt: 460 tokens, 862.170 tokens-per-sec\n",
      "Generation: 135 tokens, 68.472 tokens-per-sec\n",
      "Peak memory: 20.665 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nTo convert USD to Euros, I need to use the 'convert_currency' function from the provided tools.  I need to provide the amount to convert, the currency to convert from (USD), and the currency to convert to (Euros).  I should also make sure the amount is a number.\\n</think>\\n\\n<tool_call>\\n{\\n  'name': 'convert_currency',\\n  'arguments': {\\n    'amount': 500,\\n    'from_currency': 'USD',\\n    'to_currency': 'EUR'\\n  }\\n}\\n</tool_call> \\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model_lora, tokenizer, prompt=prompt, verbose=True, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
